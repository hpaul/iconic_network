{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.style as style\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = sqlContext.read.parquet('./data/authors')\n",
    "co_networks = sqlContext.read.parquet('./data/coauthors')\n",
    "collaborations = sqlContext.read.parquet('./data/collaborations')\n",
    "collaborations = collaborations.withColumn('published', F.to_date(F.col('published')))\n",
    "collaborations = collaborations.withColumn('year', F.year(F.col('published')))\n",
    "collaborations = collaborations.withColumnRenamed('author_id', 'auth_id')\n",
    "collaborations = collaborations.withColumn('auth_id', collaborations.auth_id.cast('bigint'))\n",
    "ego_alters = sqlContext.read.parquet('./data/ego_alters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_build = pd.read_csv('./networks_to_build.csv')\n",
    "to_build['id'] = to_build['id'].astype(str)\n",
    "to_build = to_build.sort_values(by=['country_origin'])\n",
    "to_build['coauthors_count'] = to_build['co_list'].map(lambda x: len(ast.literal_eval(x)))\n",
    "# Get missing ego networks\n",
    "ego_publications = collaborations.filter(collaborations.auth_id.isin(list(to_build['id'].values))).select(collaborations.auth_id.alias('ego_id'),collaborations.abs_id.alias('todel1'))\n",
    "ego_publications = ego_publications.join(collaborations, ego_publications.todel1 == collaborations.abs_id, 'inner')\n",
    "missing_data = ego_publications.filter(ego_publications.ego_id == ego_publications.auth_id).groupby([ego_publications.ego_id, ego_publications.auth_id]).count().select('ego_id', 'count').toPandas()\n",
    "#to_build = to_build[~to_build.id.isin(list(missing_data['ego_id'].values))]\n",
    "missing_data.to_csv('./ego_colls.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_build.groupby('EGO_COUNTRY').agg(['count', 'min', 'max', 'mean', 'std']).to_excel('./export/phys/ego_networks_description.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\n",
    "    'figure.figsize':(12,14),\n",
    "    'font.size':20,\n",
    "    'axes.titlesize':20,\n",
    "    'axes.labelsize':20,\n",
    "})\n",
    "#print(plt.rcParams.keys())\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "# Show each observation with a scatterplot\n",
    "sns.stripplot(x=\"cited_by_count\", y=\"country_origin\",\n",
    "              data=to_build, dodge=True, jitter=True,\n",
    "              alpha=.30, size=7,color='.30', zorder=1)\n",
    "\n",
    "\n",
    "g = sns.boxplot(x=\"cited_by_count\", y=\"country_origin\", data=to_build, fliersize=0, boxprops=dict(facecolor=(0,0,0,0), linewidth=2))\n",
    "xlabels = ['{:,.0f}'.format(x) + 'K' for x in g.get_xticks()/1000]\n",
    "g.set_xticklabels(xlabels)\n",
    "\n",
    "g.set_xlabel(\"Acumulated Citations\",fontsize=14)\n",
    "g.set_ylabel(\"Origin country of author\",fontsize=20)\n",
    "sns.despine(offset=20, left=True, bottom=True) \n",
    "g.get_figure().savefig(\"./export/phys/ego_networks_distribution.png\", format='png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = sqlContext.read.parquet('./data/authors')\n",
    "co_networks = sqlContext.read.parquet('./data/coauthors')\n",
    "collaborations = sqlContext.read.parquet('./data/collaborations')\n",
    "collaborations = collaborations.withColumn('published', F.to_date(F.col('published')))\n",
    "collaborations = collaborations.withColumn('year', F.year(F.col('published')))\n",
    "collaborations = collaborations.withColumnRenamed('author_id', 'auth_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_publications = collaborations.filter(collaborations.auth_id.isin(list(to_build['id'].values))).select(collaborations.auth_id.alias('ego_id'),collaborations.abs_id.alias('todel1'))\n",
    "ego_publications = ego_publications.join(collaborations, ego_publications.todel1 == collaborations.abs_id, 'inner')\n",
    "ego_coauthors = ego_publications.dropDuplicates(['ego_id', 'auth_id']).groupby([ego_publications.ego_id]).count()\n",
    "ego_coauthors = ego_coauthors.toPandas()\n",
    "# Combine networks to get all info about hem\n",
    "ego_coauthors = to_build.merge(ego_coauthors, how='left', left_on='id', right_on='ego_id')\n",
    "ego_coauthors.drop(['co_list', 'ego_id'], axis=1, inplace=True)\n",
    "# Get correct coauthor count of ego network\n",
    "ego_coauthors['coauthors'] = ego_coauthors['count']\n",
    "ego_coauthors.drop(['coauthors_count','count'], axis=1, inplace=True)\n",
    "ego_coauthors.to_excel('./export/phys/ego_coauthor_description.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute network desity\n",
    "def compute_network_density():\n",
    "    networks = list(to_build['id'].values)\n",
    "    uniqueauths = [a.author_id for a in co_networks.filter(co_networks.network_id.isin(networks)).dropDuplicates(['author_id']).select('author_id').collect()]\n",
    "      \n",
    "    networks = co_networks.filter(co_networks.network_id.isin(networks)).select(co_networks.network_id, co_networks.author_id.alias('source'))\n",
    "    ego_publications = networks \\\n",
    "        .join(collaborations, networks.source == collaborations.auth_id.alias('todel1'), 'inner') \\\n",
    "        .select(networks.network_id, networks.source, collaborations.abs_id.alias('todel2'))\n",
    "    \n",
    "    network = ego_publications \\\n",
    "        .join(collaborations, ego_publications.todel2 == collaborations.abs_id.alias('todel3'), how='inner') \\\n",
    "        .drop('todel1', 'todel2', 'todel3') \\\n",
    "        .select(ego_publications.network_id, ego_publications.source, collaborations.auth_id.alias('target'))\n",
    "    \n",
    "    network = network.withColumn('source', network.source.cast('bigint'))\n",
    "    network = network.withColumn('target', network.target.cast('bigint'))\n",
    "\n",
    "    network = network.filter(network.source != network.target) \\\n",
    "        .filter(network.source.isin(list(to_build['id'].values)) == False) \\\n",
    "        .filter(network.target.isin(list(to_build['id'].values)) == False)\n",
    "    \n",
    "    network = network.groupby('network_id') \\\n",
    "        .agg(F.count('source').alias('total_edges'), F.array_union(F.collect_set('source'),F.collect_set('target')).alias('total_nodes'))\n",
    "    \n",
    "    network = network.toPandas()\n",
    "    network['total_nodes'] = network['total_nodes'].map(lambda x: len(x))\n",
    "\n",
    "    def density(row):\n",
    "        possible_edges = (row['total_nodes']*(row['total_nodes']-1))/2\n",
    "        return row['total_edges']/possible_edges\n",
    "    \n",
    "    network['density'] = network.apply(lambda row: density(row), axis=1)\n",
    "    network.to_excel('./export/phys/ego_networks_density.xlsx')\n",
    "\n",
    "compute_network_density() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths = co_networks.filter(co_networks.network_id.isin(list(to_build['id'].values))) \\\n",
    "    .join(authors, co_networks.author_id == authors.id, 'left') \\\n",
    "    .select(co_networks.network_id,co_networks.author_id, authors.country)\n",
    "\n",
    "# auth_articles = auths \\\n",
    "#     .join(collaborations, auths.author_id == collaborations.auth_id, 'inner') \\\n",
    "#     .filter(collaborations.year > 2006)\n",
    "\n",
    "auths_per_year = auth_articles \\\n",
    "    .groupBy(auth_articles.country) \\\n",
    "    .pivot('year', range(2007,2018)).agg(F.count('abs_id')).toPandas()\n",
    "\n",
    "auths_per_year.to_csv('./per_country_year_article.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths_per_year = auth_articles \\\n",
    "    .dropDuplicates(['author_id', 'year']) \\\n",
    "    .groupBy(auth_articles.country) \\\n",
    "    .pivot('year', range(2007,2018)).agg(F.count('author_id')).toPandas()\n",
    "\n",
    "auths_per_year.to_csv('./per_country_year_authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_network(name):\n",
    "    networks = list(to_build[to_build.country_origin == name]['id'].values)\n",
    "    \n",
    "    auths = co_networks.filter(co_networks.network_id.isin(networks)).dropDuplicates(['author_id']) \\\n",
    "        .select(co_networks.author_id.alias('source'))\n",
    "    \n",
    "    articles = auths \\\n",
    "        .join(collaborations, auths.source == collaborations.auth_id.alias('todel1'), 'inner') \\\n",
    "        .select(auths.source, collaborations.abs_id.alias('todel2'))\n",
    "\n",
    "    big_network = articles \\\n",
    "        .join(collaborations, articles.todel2 == collaborations.abs_id, how='inner') \\\n",
    "        .select(articles.source,collaborations.abs_id.alias('todel3'),collaborations.auth_id.alias('target'), collaborations.year) \\\n",
    "        .filter(articles.source != collaborations.auth_id)\n",
    " \n",
    "    grouped = big_network.filter(big_network.year > 2006) \\\n",
    "        .groupby([big_network.source, big_network.target, big_network.year]).agg(F.count(F.col('todel3')).alias('weight'))\n",
    "    \n",
    "    se_data = grouped.join(authors, grouped.source == authors.id, how='left') \\\n",
    "        .select(grouped.source, grouped.target, grouped.year, grouped.weight, authors.country.alias('source_country'), authors.cited_by_count.alias('source_citations'))\n",
    "    \n",
    "    te_data = se_data.join(authors, grouped.target == authors.id, how='left') \\\n",
    "        .select(se_data.source, se_data.target, se_data.year, se_data.weight, se_data.source_country, se_data.source_citations, \\\n",
    "               authors.country.alias('target_country'), authors.cited_by_count.alias('target_cits'))\n",
    "    \n",
    "    def edge_type(scountry, tcountry):\n",
    "        etype = 'missing'\n",
    "        # domestic types\n",
    "        if scountry == name and tcountry == name:\n",
    "            etype = 'domestic'\n",
    "        \n",
    "        if scountry != name and tcountry == name:\n",
    "            etype = 'intradomestic'\n",
    "            \n",
    "        if scountry == name and tcountry != name:\n",
    "            etype = 'intradomestic'\n",
    "        \n",
    "        if scountry != name and tcountry != name:\n",
    "            etype = 'nondomestic'\n",
    "            \n",
    "        return etype\n",
    "            \n",
    "        \n",
    "    check_type = F.udf(edge_type, StringType()) \n",
    "    te_data = te_data.withColumn('type', check_type('source_country', 'target_country'))\n",
    "    return te_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = time.time()\n",
    "\n",
    "for country in to_build['country_origin'].unique():\n",
    "    start = time.time()\n",
    "    slug = slugify(country)\n",
    "    data = country_network(country)\n",
    "    #data.write.parquet('./data/per_country/{}'.format(slug))\n",
    "    print('{}: Took (s): {}'.format(slug, time.time() - start))\n",
    "    \n",
    "print('Total Took (s): {}'.format(time.time() - initial))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
