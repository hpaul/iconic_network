{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.style as style\n",
    "from slugify import slugify\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.config(\"spark.driver.memory\", \"4g\").getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_name(data):\n",
    "    countries = ['Afghanistan', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua And Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Bosnia And Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic Of The', 'Cook Islands', 'Costa Rica', \"Cote D'ivoire\", 'Croatia', 'Cuba', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'East Timor', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guinea', 'Guinea-bissau', 'Guyana', 'Haiti', 'Heard Island And Mcdonald Islands', 'Holy See (vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic Of', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakstan', 'Kenya', 'Kiribati', \"Korea, Democratic People's Republic Of\", 'Korea, Republic Of', 'Kosovo', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libyan Arab Jamahiriya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macau', 'Macedonia, The Former Yugoslav Republic Of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States Of', 'Moldova, Republic Of', 'Monaco', 'Mongolia', 'Montserrat', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'Netherlands Antilles', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Reunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Helena', 'Saint Kitts And Nevis', 'Saint Lucia', 'Saint Pierre And Miquelon', 'Saint Vincent And The Grenadines', 'Samoa', 'San Marino', 'Sao Tome And Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia And The South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard And Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan, Province Of China', 'Tajikistan', 'Tanzania, United Republic Of', 'Thailand', 'Togo', 'Tokelau', 'Tonga', 'Trinidad And Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks And Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Viet Nam', 'Virgin Islands, British', 'Virgin Islands, U.s.', 'Wallis And Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe', 'Moldova', 'South Korea']\n",
    "    co = data or ''\n",
    "    \n",
    "    computed = ''\n",
    "    for name in co.split(' '):\n",
    "        computed = (\"{} {}\".format(computed, name)).strip()\n",
    "        if computed in countries:\n",
    "            return computed\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "author_country = F.udf(lambda x: get_country_name(x), StringType())\n",
    "\n",
    "authors = sqlContext.read.parquet('./data/authors')\n",
    "authors = authors.withColumn('country', author_country('country'))\n",
    "\n",
    "collaborations = sqlContext.read.parquet('./data/collaborations')\n",
    "collaborations = collaborations.withColumn('published', F.to_date(F.col('published')))\n",
    "collaborations = collaborations.withColumn('year', F.year(F.col('published')))\n",
    "collaborations = collaborations.withColumnRenamed('author_id', 'auth_id')\n",
    "collaborations = collaborations.withColumn('auth_id', collaborations.auth_id.cast('bigint'))\n",
    "collaborations = collaborations.filter(F.col('year') > 2006)\n",
    "ego_alters = sqlContext.read.parquet('./data/ego_alters')\n",
    "# Exclude articles which have errors\n",
    "collaborations = collaborations.filter(~(collaborations.abs_id.isin([85032509284,80053369953])))\n",
    "# Compute authors citations based on downloaded papers\n",
    "# Because not all articles for authors were scraped we need to base our analysis on grounded data, not missing\n",
    "authors = authors.join(collaborations, collaborations.auth_id == authors.id, 'INNER') \\\n",
    "        .groupby([authors.id]) \\\n",
    "        .agg( \\\n",
    "            F.countDistinct('abs_id').alias('PAPERS'), \\\n",
    "            F.first('cited_by_count').alias('CITATIONS_SCOPUS'), \\\n",
    "            F.sumDistinct('cited_by').alias('CITATIONS'), \\\n",
    "            F.first('cat').alias('cat'), \\\n",
    "            F.first('university').alias('university'), \\\n",
    "            F.first('city').alias('city'), \\\n",
    "            F.first('country').alias('country'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count for each author in database the number of papers where he participated and gather first 1200 most productive from EU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', \n",
    "             'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', \n",
    "             'Ireland', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands',\n",
    "             'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'United Kingdom']\n",
    "\n",
    "cat_name = 'MEDI'\n",
    "\n",
    "most_productive = authors.orderBy(F.col('PAPERS').desc()) \\\n",
    "    .filter(F.col('country').isin(countries)) \\\n",
    "    .filter(F.col('cat').like('[\"{}\"]'.format(cat_name))) \\\n",
    "    .limit(1200) \\\n",
    "    .select( \\\n",
    "        authors.cat.alias('cat'), \\\n",
    "        authors.id.alias('EGO_ID'), \\\n",
    "        authors.country.alias('EGO_COUNTRY'), \\\n",
    "        authors.city.alias('EGO_CITY'), \\\n",
    "        authors.university.alias('EGO_UNIVERSITY'), \\\n",
    "        authors['PAPERS'].alias('EGO_PAPERS'), \\\n",
    "        authors['CITATIONS'].alias('EGO_CITATIONS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get papers of most productive authors from selected domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_productive_papers = most_productive \\\n",
    "    .join(collaborations, F.col('EGO_ID') == collaborations.auth_id, 'inner') \\\n",
    "    .select( \\\n",
    "        'EGO_ID', 'EGO_COUNTRY', 'EGO_PAPERS', 'EGO_CITATIONS', \\\n",
    "        F.col('abs_id').alias('paper_id'), 'year'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all authors of these papers, meaning coauthors of each ego. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_productive_coauthors = most_productive_papers \\\n",
    "    .filter(F.col('year') >= 2007) \\\n",
    "    .join(collaborations, most_productive_papers.paper_id == collaborations.abs_id, 'INNER') \\\n",
    "    .select( \\\n",
    "        'EGO_ID', \\\n",
    "        F.col('auth_id').alias('source_id'), \\\n",
    "        F.col('abs_id').alias('paper_id2')\n",
    "    )\n",
    "\n",
    "#most_productive_coauthors = most_productive_coauthors.filter(F.col('EGO_ID') != F.col('source_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a list of all unique authors. We need this for further joining. Because we only want to get connections between egos coauthors, the list should not contain co-authors of co-authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect connections between these authors, filter out these which we not interesed about, and duplicated connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_productive_conns = most_productive_coauthors \\\n",
    "    .join(collaborations, most_productive_coauthors.paper_id2 == collaborations.abs_id, 'INNER') \\\n",
    "    .select('source_id', F.col('auth_id').alias('target_id'), F.col('abs_id').alias('paper_id3'), 'year')\n",
    "\n",
    "most_productive_conns = most_productive_conns \\\n",
    "    .filter(F.col('source_id') != F.col('target_id'))\n",
    "\n",
    "most_productive_conns = most_productive_conns.select( \\\n",
    "        F.least(F.col('source_id'), F.col('target_id')).alias('source_id'), \\\n",
    "        F.greatest(F.col('source_id'), F.col('target_id')).alias('target_id'), 'paper_id3', 'year')\n",
    "\n",
    "most_productive_conns = most_productive_conns.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save checkpoint with all connection betweeen coauthors from selected category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_productive_conns = most_productive_conns.join(authors, most_productive_conns.source_id == authors.id, 'LEFT') \\\n",
    "    .select('source_id', F.col('country').alias('source_country'), \\\n",
    "            'target_id', 'paper_id3', 'year')\n",
    "\n",
    "most_productive_conns = most_productive_conns.join(authors, most_productive_conns.target_id == authors.id, 'LEFT') \\\n",
    "    .select('source_id', 'source_country', \\\n",
    "            'target_id', F.col('country').alias('target_country'), \\\n",
    "            'paper_id3', 'year')\n",
    "\n",
    "\n",
    "# Filter out authors without country\n",
    "most_productive_conns = most_productive_conns.filter(F.col('source_country').isNotNull())\n",
    "most_productive_conns = most_productive_conns.filter(F.col('target_country').isNotNull())\n",
    "\n",
    "\n",
    "most_productive_conns.write.parquet('./data/{}_authors_network'.format(cat_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data about these authors and build final aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_productive_conns = sqlContext.read.parquet('./data/{}_authors_network'.format(cat_name))\n",
    "\n",
    "country_network = most_productive_conns \\\n",
    "    .groupby(['source_country', 'target_country', 'year']) \\\n",
    "    .agg( \\\n",
    "        F.collect_set('source_id').alias('source_uniq'), \\\n",
    "        F.collect_set('target_id').alias('target_uniq'), \\\n",
    "        F.count('paper_id3').alias('edges') \\\n",
    "    )\n",
    "\n",
    "country_network = country_network.withColumn('sourceid', F.regexp_replace(F.concat('source_country', 'year'), r'[\\s_-]+', '-'))\n",
    "country_network = country_network.withColumn('targetid', F.regexp_replace(F.concat('target_country', 'year'), r'[\\s_-]+', '-'))\n",
    "country_network = country_network.withColumn('uniq_country_id', F.sort_array(F.array('source_country', 'target_country', 'year')))\n",
    "\n",
    "country_conns = country_network.select('uniq_country_id', 'sourceid', 'targetid', 'source_country', 'target_country', 'year', 'edges')\n",
    "country_conns = country_conns.groupby('uniq_country_id') \\\n",
    "    .agg( \\\n",
    "        F.first('sourceid').alias('sourceid'), \\\n",
    "        F.first('targetid').alias('targetid'), \\\n",
    "        F.first('source_country').alias('source_country'), \\\n",
    "        F.first('target_country').alias('target_country'), \\\n",
    "        F.first('year').alias('year'), \\\n",
    "        F.sum('edges').alias('edges') \\\n",
    "    )\n",
    "\n",
    "country_conns = country_conns.select('sourceid', 'targetid', 'source_country', 'target_country', 'year', 'edges')\n",
    "\n",
    "# Country of interest (only these will be saved)\n",
    "europe_countries = ['Albania','Andorra','Armenia','Austria','Azerbaijan','Belarus','Belgium','Bosnia and Herzegovina','Bulgaria','Croatia','Cyprus','Czechia','Denmark','Estonia','Finland','France','Georgia','Germany','Greece','Hungary','Iceland','Ireland','Italy','Kazakhstan','Kosovo','Latvia','Liechtenstein','Lithuania','Luxembourg','Malta','Moldova','Monaco','Montenegro','Netherlands','Macedonia','Norway','Poland','Portugal','Romania','Russia','San Marino','Serbia','Slovakia','Slovenia','Spain','Sweden','Switzerland','Turkey','Ukraine','United Kingdom','Vatican City']\n",
    "country_conns = country_conns.filter(F.col('source_country').isin(europe_countries))\n",
    "country_conns = country_conns.filter(F.col('target_country').isin(europe_countries))\n",
    "\n",
    "country_conns.write.csv(\"./data/country_networks/{}_conns\".format(cat_name), compression=None, nullValue='N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect unique nodes from edges, collect papers and citations data about them, exploding the unique array, then export it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique authors for each country from source column\n",
    "country_source_nodes = country_network.groupby(['source_country', 'year']) \\\n",
    "    .agg( \\\n",
    "        F.first('sourceid').alias('sourceid'), \\\n",
    "        F.collect_list('source_uniq').alias('source_authors'), \\\n",
    "    )\n",
    "country_source_nodes = country_source_nodes.withColumn('source_authors', F.array_distinct(F.flatten('source_authors')))\n",
    "\n",
    "# Unique authors for each country from target column\n",
    "country_target_nodes = country_network.groupby(['target_country', 'year']) \\\n",
    "    .agg( \\\n",
    "        F.first('targetid').alias('targetid'), \\\n",
    "        F.collect_list('target_uniq').alias('target_authors'), \\\n",
    "    )\n",
    "country_target_nodes = country_target_nodes.withColumn('target_authors', F.array_distinct(F.flatten('target_authors')))\n",
    "\n",
    "\n",
    "country_nodes = country_source_nodes.join(country_target_nodes.alias('t'), country_source_nodes.sourceid == country_target_nodes.targetid, 'LEFT') \\\n",
    "    .select(F.col('sourceid').alias('nodeid'), F.col('source_country').alias('country'), 't.year', \\\n",
    "            F.array_distinct(F.flatten(F.array('source_authors', 'target_authors'))).alias('uniq_authors'))\n",
    "    \n",
    "\n",
    "country_nodes = country_nodes.select('*', F.explode('uniq_authors').alias('auth_id'))\n",
    "country_nodes = country_nodes.join(authors.alias('ad'), country_nodes.auth_id == authors.id, 'LEFT') \\\n",
    "    .select('nodeid', 'ad.country', 'year', 'auth_id', 'ad.PAPERS', 'ad.CITATIONS')\n",
    "\n",
    "country_nodes = country_nodes.groupby(['nodeid']) \\\n",
    "    .agg( \\\n",
    "        F.first('country').alias('country'), \\\n",
    "        F.first('year').alias('year'), \\\n",
    "        F.count('auth_id').alias('nodes'), \\\n",
    "        F.sum('PAPERS').alias('cumulated_papers'), \\\n",
    "        F.sum('CITATIONS').alias('cumulated_citations') \\\n",
    "    )\n",
    "\n",
    "country_nodes = country_nodes.join(country_network.alias('network'), (country_nodes.nodeid == country_network.sourceid) & (country_nodes.nodeid == country_network.targetid)) \\\n",
    "    .select('nodeid', 'country', 'network.year', 'nodes', 'cumulated_papers', 'cumulated_citations', 'network.edges')\n",
    "\n",
    "# Country of interest (only these will be saved)\n",
    "europe_countries = ['Albania','Andorra','Armenia','Austria','Azerbaijan','Belarus','Belgium','Bosnia and Herzegovina','Bulgaria','Croatia','Cyprus','Czechia','Denmark','Estonia','Finland','France','Georgia','Germany','Greece','Hungary','Iceland','Ireland','Italy','Kazakhstan','Kosovo','Latvia','Liechtenstein','Lithuania','Luxembourg','Malta','Moldova','Monaco','Montenegro','Netherlands','Macedonia','Norway','Poland','Portugal','Romania','Russia','San Marino','Serbia','Slovakia','Slovenia','Spain','Sweden','Switzerland','Turkey','Ukraine','United Kingdom','Vatican City']\n",
    "country_nodes = country_nodes.filter(F.col('country').isin(europe_countries))\n",
    "\n",
    "country_nodes.write.csv(\"./data/country_networks/{}_nodes\".format(cat_name), compression=None, nullValue='N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "down here things should be rewrited and refactored\n",
    "\n",
    "old code which could be simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_network(country):\n",
    "    i_edges = sqlContext.read.parquet(os.path.abspath('./data/per_country/{}'.format(slugify(country))))\n",
    "    # Select only edges between networks of most cited authors in country \n",
    "    nodes = set(i_edges.select(F.collect_set(\"source\").alias(\"source\")).first()[\"source\"])\n",
    "    data = i_edges.filter(i_edges.target.isin(nodes))\n",
    "    \n",
    "    authors_details = authors.filter(authors.id.isin(nodes)).select(authors.id, authors.country, authors.cited_by_count, authors.agg_citations)\n",
    "    edges = data.groupby([i_edges.source,i_edges.target]).agg(F.sum('weight').alias('weight'), F.first('type').alias('type'))\n",
    "    \n",
    "    def relation_type(source, target, egos):\n",
    "        etype = 'missing'\n",
    "        source_is_ego = str(source) in egos\n",
    "        target_is_ego = str(target) in egos\n",
    "        # domestic types\n",
    "        if source_is_ego and target_is_ego:\n",
    "            etype = 'ego-ego'\n",
    "        \n",
    "        if source_is_ego and not target_is_ego:\n",
    "            etype = 'ego-coauthor'\n",
    "            \n",
    "        if not source_is_ego and target_is_ego:\n",
    "            etype = 'coauthor-ego'\n",
    "        \n",
    "        if not source_is_ego and not target_is_ego:\n",
    "            etype = 'coauthor-coauthor'\n",
    "            \n",
    "        return etype\n",
    "    \n",
    "    def apply_relation(egos):\n",
    "        return F.udf(lambda s,t: relation_type(s,t,egos), StringType())\n",
    "\n",
    "    edges = edges.withColumn('relation', apply_relation(to_build['id'].values)(F.col('source'), F.col('target')))\n",
    "    \n",
    "    \n",
    "    authpan = authors_details.toPandas()\n",
    "    authpan['ego'] = authpan['id'].map(lambda x: 'Yes' if str(x) in to_build['id'].values else 'No')\n",
    "    authpan['type'] = authpan['country'].map(lambda x: 'Intern' if x == country else 'Extern')\n",
    "    authpan['cited_by_count'] = authpan['cited_by_count'].fillna(authpan.agg_citations).astype(int)\n",
    "    authpan.drop(['agg_citations'], axis=1, inplace=True)\n",
    "    \n",
    "    nodes_path = './data/country_networks/{}_nodes.csv'.format(slugify(country))\n",
    "    edges_path = './data/country_networks/{}_edges.csv'.format(slugify(country))\n",
    "    \n",
    "    authpan.to_csv(nodes_path, index=False)\n",
    "    edges.toPandas().to_csv(edges_path,index=False)\n",
    "    #return authpan, edges\n",
    "\n",
    "\n",
    "def country_network_authors(country):\n",
    "    i_edges = sqlContext.read.parquet(os.path.abspath('./data/per_country/{}'.format(slugify(country))))\n",
    "    # Select only edges between networks of most cited authors in country \n",
    "    nodes = set(i_edges.select(F.collect_set(\"source\").alias(\"source\")).first()[\"source\"])\n",
    "    data = i_edges.filter(i_edges.target.isin(nodes))\n",
    "    \n",
    "    authors_details = authors.filter(authors.id.isin(nodes)).select(authors.id, authors.country, authors.cited_by_count, authors.agg_citations)\n",
    "    authpan = authors_details.toPandas()\n",
    "    authpan['type'] = authpan['country'].map(lambda x: 'Intern' if x == country else 'Extern')\n",
    "    authpan['cited_by_count'] = authpan['cited_by_count'].fillna(authpan.agg_citations).astype(int)\n",
    "    \n",
    "    return authpan\n",
    "\n",
    "def country_network_edges(country):\n",
    "    i_edges = sqlContext.read.parquet(os.path.abspath('./data/per_country/{}'.format(slugify(country))))\n",
    "    # Select only edges between networks of most cited authors in country \n",
    "    nodes = set(i_edges.select(F.collect_set(\"source\").alias(\"source\")).first()[\"source\"])\n",
    "    data = i_edges.filter(i_edges.target.isin(nodes))\n",
    "    edges = data.groupby([i_edges.source,i_edges.target]).agg(F.sum('weight').alias('weight'), F.first('type').alias('type'))\n",
    "    \n",
    "    return edges.groupby(edges.type).agg(F.count('type').alias('count'))\n",
    "    \n",
    "    \n",
    "def country_per_year_articles(country):\n",
    "    i_edges = sqlContext.read.parquet(os.path.abspath('./data/per_country/{}'.format(slugify(country))))\n",
    "    # Select only edges between networks of most cited authors in country \n",
    "    nodes = set(i_edges.select(F.collect_set(\"source\").alias(\"source\")).first()[\"source\"])\n",
    "    data = i_edges.filter(i_edges.target.isin(nodes))\n",
    "    a_details = authors.filter(authors.id.isin(nodes)).select(authors.id, authors.country)\n",
    "    author_articles = a_details.join(collaborations, a_details.id == collaborations.auth_id, 'inner') \\\n",
    "        .groupby([a_details.id, collaborations.year]).agg(F.count('abs_id').alias('count'))\n",
    "    \n",
    "    return author_articles\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build csv files\n",
    "#country_network('Poland')\n",
    "for country in countries:\n",
    "    country_network(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_edges = pd.DataFrame(columns=['Country'])\n",
    "\n",
    "for cny in countries:\n",
    "    data = country_network_edges(cny)\n",
    "    data = data.toPandas().set_index('type').T.to_dict('list')\n",
    "    \n",
    "    country_edges = country_edges.append({\n",
    "        'Country': cny,\n",
    "        'domestic': data['domestic'][0],\n",
    "        'nondomestic': data['nondomestic'][0],\n",
    "        'intradomestic': data['intradomestic'][0],\n",
    "    },ignore_index=True)\n",
    "    \n",
    "country_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = pd.DataFrame(columns=['cited_by_count', 'country'])\n",
    "\n",
    "for cty in countries:\n",
    "    nodes = country_network_authors(cty)\n",
    "    nodes['country'] = cty\n",
    "    country_data = country_data.append(nodes[['cited_by_count', 'country']])\n",
    "\n",
    "country_data.rename({'cited_by_count': 'Citations'}, inPlace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = country_data[country_data.cited_by_count < 100000]\n",
    "pal = sns.cubehelix_palette(1, rot=1, light=.1)\n",
    "g = sns.FacetGrid(filtered, row=\"country\", hue=\"country\", aspect=15, height=1, palette=pal)\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"cited_by_count\", clip_on=False, shade=True, alpha=1, lw=1.5, bw=.2)\n",
    "g.map(sns.kdeplot, \"cited_by_count\", clip_on=False, color=\"w\", lw=2, bw=.2)\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.margins(x=0, y=0) \n",
    "    ax.text(-0.02, .2, label, fontweight=\"bold\", color=color,\n",
    "            ha=\"right\", va=\"center\", transform=ax.transAxes)\n",
    "    \n",
    "g.map(label, \"cited_by_count\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "g.fig.subplots_adjust(hspace=-.20)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set_xlabels('Citations')\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "g.savefig(\"./phys_citations_densities.png\", format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_edges.to_csv('./phys_country_connections_type.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_per_year = pd.DataFrame(columns=['country', 'id', 'year', 'count'])\n",
    "\n",
    "for cty in countries:\n",
    "    nodes = country_per_year_articles(cty)\n",
    "    nodes = nodes.toPandas()\n",
    "    nodes['country'] = cty\n",
    "    country_per_year = country_per_year.append(nodes[['country', 'id', 'year', 'count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_per_year['year'] = country_per_year['year'].fillna(0).astype(int)\n",
    "# country_per_year['count'] = country_per_year['count'].astype(int)\n",
    "\n",
    "country_per_year = pd.read_csv('./phys_country_articles.csv')\n",
    "country_per_year['year'] = country_per_year['year'].fillna(0).astype(int)\n",
    "country_per_year['count'] = country_per_year['count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to_save = country_per_year[country_per_year['year'] > 2006]\n",
    "# to_save = to_save[to_save['year'] < 2018]\n",
    "\n",
    "\n",
    "# to_save['year'] = to_save['year'].astype(str)\n",
    "# to_save['year'] = to_save['year'].str.replace('.0', '')\n",
    "\n",
    "\n",
    "# to_show = country_per_year[country_per_year.country == 'Polonia']\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set(rc={\n",
    "    'figure.figsize':(16,20),\n",
    "    'font.size':20,\n",
    "    'axes.titlesize':20,\n",
    "    'axes.labelsize':20,\n",
    "})\n",
    "\n",
    "\n",
    "# to_show = to_show.groupby('year').agg({ 'count': 'sum' })\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"count\", y=\"country\", data=country_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
